{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4aad44-6cbb-4eec-9aa3-1d17c8f63ce8",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "- `dropout_unit` : Obtain filtered A[l] by shutting-off the nodes with binomial(N = n^[l] * m, p = 1-keep_prob) \n",
    "- `get_L2_weight_penalty` : Obtain L2_weight_penalty by input lambd, param, and m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6a8d5-35dc-4d1b-80c9-6e09a524478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% External module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42744489-9baa-4745-b167-e91d4c1e1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_unit(A,keep_prob):\n",
    "    \"\"\"Implement dropout to activation node output\n",
    "    \n",
    "    Arguments\n",
    "    -------------------\n",
    "    A --- Activation node output\n",
    "    keep_prob --- The proportion of non-shut-off units in the layer\n",
    "    \n",
    "    Returns\n",
    "    -------------------\n",
    "    A --- Filtered Activation node output by Dropout filter\n",
    "    \"\"\"\n",
    "    \n",
    "    D = np.random.rand(A.shape[0],A.shape[1]) < keep_prob   # Dropout filter\n",
    "    A = np.multiply(A,D)\n",
    "    \n",
    "    A /= keep_prob #Inverted dropout\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc6c2e-0f31-492d-9049-2362308aa495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2_weight_penalty(lambd : float, param : list, m : int) -> float:\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    -----------------------------------------\n",
    "    lambd --- float\n",
    "            regularization term \n",
    "    param --- list\n",
    "            parameter W^[1], B^[1],... W^[L], B^[L]\n",
    "    \n",
    "    m --- int\n",
    "           number of observation\n",
    "        \n",
    "    Return :\n",
    "    -----------------------------------------\n",
    "    L2_weight_penalty --- float\n",
    "    \n",
    "    \"\"\"\n",
    "    L:int = len(param) // 2\n",
    "    L2_weight_penalty = 0\n",
    "    \n",
    "    for l in range(1,L+1): #summation of square of Frobenius weight norm for every layer\n",
    "        W = param['W'+str(l)]\n",
    "        L2_weight_penalty += np.sum(np.square(W))     \n",
    "    \n",
    "    L2_weight_penalty *= (lambd / (2*m) ) \n",
    "    \n",
    "    return L2_weight_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe778855-545f-4b65-b87f-4372507b0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2_weight_decay(lambd : float, param : list, m : int) -> np.array:\n",
    "    \"\"\"\n",
    "    Return L2 weight decay of a single layer\n",
    "    \n",
    "    Arguments:\n",
    "    -----------------------------------------\n",
    "    lambd --- float\n",
    "            regularization term \n",
    "    param --- list\n",
    "            parameter W^[1], B^[1],... W^[L], B^[L]\n",
    "    \n",
    "    m --- int\n",
    "           number of observation\n",
    "        \n",
    "    Return :\n",
    "    -----------------------------------------\n",
    "    L2_weight_decay --- np.array\n",
    "            How much parameter was decays\n",
    "    \n",
    "    \"\"\"    \n",
    "    pass\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
