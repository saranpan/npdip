{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff55e72-d5e1-4c42-af71-24a9f17b1ea4",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Components\n",
    "\n",
    "- A.K.A. Components of Deep-L-Layer \n",
    "> consists of \n",
    "> 1. L_model_forward (Forward Propagation) to return output\n",
    "> 2. L_model_backward (Backward Propagation) to return error\n",
    "\n",
    "Then use those errors of every parameter (known as gradient of cost function with respect to every parameter) to update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e66b19-67c2-453b-be09-507b083b887a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img_framework/mlp_architecture.jpg\" alt=\"multilayer perceptron architecture\" width=\"600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c104916-fad3-4a65-9b1a-4e5c9250e4d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e5e712-e77a-4761-86c0-14bfdeb73331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79675a5f-9c51-4af8-b5ca-1a102a92887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation import *\n",
    "from regularization import dropout_unit\n",
    "\n",
    "# %% External module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2a55f-ba82-49e9-8f7e-7d6f7ba6b085",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbd416-c584-4979-8899-78021f9c9e4e",
   "metadata": {},
   "source": [
    "0. Hyperparameter : Hidden Activation function , Output activation function\n",
    "1. Input : A[0], parameter (dictionary of W[l], b[l])\n",
    "2. Output : A[L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a29963e6-e9bd-484a-802c-9421a134f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['linear_forward', 'linear_activation_forward', 'L_model_forward', \n",
    "           'linear_backward','linear_activation_backward', 'L_model_backward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f87a990-b290-4c11-bef7-83e73e0d8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"Linear Forward unit\n",
    "    - Retrieve A_prev , W, b, and turn them into Z (with cache)\n",
    "    \n",
    "    Argument\n",
    "    ----------    \n",
    "    1. A_prev --- Activation node of the previous layer A[l-1]\n",
    "    2. W --- Weight of layer l\n",
    "    3. b --- Bias of layer l\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    1. Z --- Output Z of layer l \n",
    "    2. caches --- cache of Linear forward Unit\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (A_prev,W,b)      # A :for dZ, W for dA & to get updating, b for updating , dA for dZ\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3734e4-9f74-439c-9fe2-bcc2d86cf652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_function):\n",
    "    \"\"\"Linear Forward unit\n",
    "    - Dependencies of linear_forward and turn Z into A (with linear cache & activation cache)\n",
    "    \n",
    "    Argument\n",
    "    ----------    \n",
    "    1. A_prev --- Activation node of the previous layer A[l-1]\n",
    "    2. W --- Weight of layer l\n",
    "    3. b --- Bias of layer l\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    1. Z --- Output Z of layer l \n",
    "    2. caches --- cache of Linear forward Unit and Activation function\n",
    "    \"\"\"\n",
    "    allowed_activation_function = {'sigmoid' : sigmoid,\n",
    "                                  'tanh': tanh,\n",
    "                                  'relu':relu,\n",
    "                                  'leakyrelu' : leakyrelu,\n",
    "                                  'linear' : linear}\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    g = allowed_activation_function[activation_function]\n",
    "    A = g(Z)\n",
    "    \n",
    "    activation_cache = Z\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (activation_cache, linear_cache)  # (Z, (A_prev,W,b))\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "287e5266-a94e-499b-8abd-4c31b808e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, param, hidden_activation_function, output_activation_function,**kwargs):   # Matter on Dropout\n",
    "    \"\"\"Forward propagation model from input to output layer\n",
    "       Apply parameter to the input X to return the Activation Output \n",
    "    \n",
    "    Argument\n",
    "    ----------    \n",
    "    1. X --- Input denoted as A[0]\n",
    "    2. param --- Weight and Bias of every layer\n",
    "                where its key must be {'W1','b1','W2','b2',...'WL','bL'}\n",
    "                      its value must be numpy array with size n_l * n_l-1\n",
    "                      \n",
    "    3. hidden_activation_function --- the activation function for hidden layer \n",
    "    4. output_activation_function --- the activation function for output layer \n",
    "                                    Binary Classication : sigmoid\n",
    "                                    Regression : linear\n",
    "  \n",
    "    Keyword Argument\n",
    "    ----------   \n",
    "    1. keep_prob_sequence --- When the regularization technique is dropout\n",
    "    \n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    1. AL --- Output A[L] from the propagation (Z[L] with sigmoid activation function)\n",
    "    2. caches --- the cache of every layer l \n",
    "    \"\"\"\n",
    "   \n",
    "    keep_prob_sequence = kwargs.get('keep_prob_sequence',None)\n",
    "\n",
    "    A = X\n",
    "    L = (len(param) // 2)  # param stores the weight and bias for L layer, hence len(param) = 2L\n",
    "\n",
    "    caches = []\n",
    "    \n",
    "\n",
    "    # For Hidden Layer [1,2..,L-1]\n",
    "    for l in range(1,L):  # l = 1,2,..,L-1\n",
    "        A_prev = A\n",
    "        W = param[\"W\" + str(l)]\n",
    "        b = param[\"b\" + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, hidden_activation_function)\n",
    "        \n",
    "        if keep_prob_sequence is not None:                #For dropout\n",
    "            A = dropout_unit(A,keep_prob_sequence[l])     \n",
    "            \n",
    "        caches.append(cache)  # append cache at layer l\n",
    "    \n",
    "    # For Output layer [L]\n",
    "    \n",
    "    A_prev = A\n",
    "    W = param[\"W\" + str(L)]\n",
    "    b = param[\"b\" + str(L)]\n",
    "    AL, cache = linear_activation_forward(A_prev, W, b, output_activation_function)\n",
    "    \n",
    "    if keep_prob_sequence is not None:     #For dropout\n",
    "        A = dropout_unit(A,keep_prob_sequence[l])\n",
    "        \n",
    "    caches.append(cache)\n",
    "\n",
    "    \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61681212-4773-47fe-86e4-d984c7e02478",
   "metadata": {},
   "source": [
    "With Forward Propagation, we got \n",
    "\n",
    "1. activation output at the last layer (AL) : \n",
    "\n",
    "<fieldset>\n",
    "    \n",
    "- data type : numpy array\n",
    "- size [1 * m]\n",
    "\n",
    "</fieldset>\n",
    "\n",
    "2. cache (for every layer) : \n",
    "\n",
    "<fieldset> \n",
    "    \n",
    "- data type : list\n",
    "- len : L \n",
    "- each element : (activation_cache, linear_cache)\n",
    "- activation cache : Z_[l]\n",
    "- linear_cache : (A_[l-1], W[l], b[l]) \n",
    "    \n",
    "</fieldset>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7530b1e-d1e1-4ee0-9597-779d0d2efc4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f14a5-0e5d-4a28-8573-ff2afc460190",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a1a96-f8ca-4f88-aad0-58648a4fed67",
   "metadata": {},
   "source": [
    "0. Hyperparameter : Hidden Activation function , Output activation function\n",
    "1. Input : A[L], cache\n",
    "2. Output : gradient of cost function of every parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77fd9fa1-e19c-4234-9352-d8c170d2b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backward Propagation Unit\n",
    "\"\"\"\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"Retreive dZ from the layer l to obtain dW,dB,dA_prev\n",
    "    Arguments\n",
    "    ----------\n",
    "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "      cache -- tuple of values (Z,(A_prev, W, b)) coming from the forward propagation in the current layer (We use only linear cache anyway)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "      dA_prev --- Gradient of the cost with respect to the activation node at the previous layer\n",
    "      dW --- Gradient of the cost with the weight in this layer\n",
    "      db --- Gradient of the cost with the bias in this layer\n",
    "    \"\"\"\n",
    "    _, linear_cache = cache  # We use only linear cache\n",
    "    (A_prev, W, b) = linear_cache  # We do not use b to obtain those 3 gradients\n",
    "\n",
    "    m = dZ.shape[1]  \n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc34fad9-d503-43ad-af6f-420d8ce3a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation_function):\n",
    "    \"\"\"Input dA to find dZ, then use dZ to obtain dW,dB,dA_prev\n",
    "    Arguments\n",
    "    ----------\n",
    "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "      cache -- tuple of values (Z,(A_prev, W, b)) coming from the forward propagation in the current layer (We use only linear cache anyway)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "      dA_prev --- Gradient of the cost with respect to the activation node at the previous layer\n",
    "      dW --- Gradient of the cost with the weight in this layer\n",
    "      db --- Gradient of the cost with the bias in this layer\n",
    "    \"\"\"\n",
    "    \n",
    "    allowed_activation_function = {'sigmoid' : dsigmoid,\n",
    "                                  'tanh': dtanh,\n",
    "                                  'relu':drelu,\n",
    "                                  'leakyrelu' : dleakyrelu,\n",
    "                                  'linear' : 1}\n",
    "    \n",
    "    activation_cache, _ = cache  # We use only activation cache\n",
    "    Z = activation_cache\n",
    "    \n",
    "    g_ = allowed_activation_function[activation_function]\n",
    "    dZ = dA * g_(Z)\n",
    "    dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803642fd-6f04-4b19-bfdf-2b305b30b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, cache, hidden_activation_function, output_activation_function):\n",
    "    \"\"\"\n",
    "    Backward propagation model from output AL to the parameter gradient of all layers\n",
    "    Apply parameter to the input X to return the Activation Output \n",
    "    \n",
    "    Arguments:\n",
    "    A --- A at the layer L\n",
    "    y --- an actual output\n",
    "    cache --- cache from the forward propagation\n",
    "    hidden_activation_function --- activation function for the hidden layer\n",
    "    output_activation_function --- activation function for the output layer\n",
    "    Return:\n",
    "     grads  -- A dictionary with the gradients\n",
    "               grads[\"dA\" + str(l)] = ...\n",
    "               grads[\"dW\" + str(l)] = ...\n",
    "               grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(cache)  # cache for each layer\n",
    "    grads = {}\n",
    "    \n",
    "    # For Output layer\n",
    "    dAL = np.divide(1 - Y, 1 - AL) - np.divide(Y, AL)  # dA_[L] : Input for the first linear activation backward  #WARNING:deprecated\n",
    "                                                        # Loss : Binary Cross Entropy\n",
    "    \n",
    "    current_cache = cache[-1] \n",
    "    dA_prev, dW, db = linear_activation_backward(dAL,current_cache,output_activation_function)\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    dA = dA_prev\n",
    "    \n",
    "    \n",
    "    # For Hidden layer [L-1, L-2...,1]\n",
    "    for l in reversed(range(1,L)): \n",
    "\n",
    "        current_cache = cache[l-1] \n",
    "        (activation_cache, linear_cache) = current_cache\n",
    "        \n",
    "        Z = activation_cache\n",
    "        a_prev, W, b = linear_cache  # Start with Z_[L] , A_[L-1], W_[L], b_[L]\n",
    "        \n",
    "        dA_prev, dW, db = linear_activation_backward(dA, current_cache, hidden_activation_function)\n",
    "\n",
    "        grads[\"dW\" + str(l)] = dW\n",
    "        grads[\"db\" + str(l)] = db\n",
    "        \n",
    "        dA = dA_prev\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b75d9-ca15-44fb-bcff-1191924dbae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fbcfa3-0742-4bd9-b23e-86d6e69c8031",
   "metadata": {},
   "source": [
    "With Backward Propagation, we got \n",
    "\n",
    "1. Gradient of cost function of all parameters : \n",
    "\n",
    "<fieldset>\n",
    "    \n",
    "- data type : dictionary\n",
    "- len : 2L (Each layer have weight and bias, so 2)\n",
    "\n",
    "- len is the equivalent as `param`\n",
    "\n",
    "</fieldset>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5c010-881b-4927-86e1-bbafc7d0d201",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8020a9-06eb-475f-9c28-2e1c9fda1973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
