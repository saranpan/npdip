{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f50107d-cecd-4694-bba1-a3abc481e263",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "- A final model in form of object built by <i>mlp_component</i> called MultilayerPerceptron\n",
    "- methods in MultilayerPerceptron have its own purpose:\n",
    "1. `__init__` : Define the required structure of mlp architecture \n",
    "2. `compile` : Specify how you want mlp to behave\n",
    "3. `fit` : fit data with the given epoch, batch size, and more\n",
    "4. `predict` : predicting input (**not test on regression, multi-classification yet** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3747278c-c74e-4b72-8faa-090e52c00ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfe2d14-5158-4b82-9caf-b5f6e5215aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from mlp_component.ipynb\n",
      "importing Jupyter notebook from activation.ipynb\n",
      "importing Jupyter notebook from regularization.ipynb\n",
      "importing Jupyter notebook from compute_cost.ipynb\n",
      "importing Jupyter notebook from losses.ipynb\n",
      "importing Jupyter notebook from update_params.ipynb\n",
      "importing Jupyter notebook from optimizer.ipynb\n",
      "importing Jupyter notebook from debug_util.ipynb\n",
      "importing Jupyter notebook from C:\\Users\\Wallik\\Desktop\\dipple\\nbdip\\utils\\py_util.ipynb\n",
      "importing Jupyter notebook from initializer.ipynb\n",
      "importing Jupyter notebook from load.ipynb\n",
      "importing Jupyter notebook from metrics.ipynb\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14892/4117248897.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitialize_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialize_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mload\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_backward_compatible\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\import_ipynb.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(self, fullname)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_transformer_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;31m# run the code in themodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_user_ns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\dipple\\nbdip\\metrics.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\dipple\\nbdip\\metrics.ipynb\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(Y, Y_pred)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from mlp_component import *                          #linear_forward ,.... linear_backward_model\n",
    "from compute_cost import *\n",
    "from update_params import *\n",
    "from debug_util import *                       # gradient_check_n\n",
    "from initializer import *\n",
    "from activation import *\n",
    "from losses import *                           # binary_cross_entropy_loss, BCE_dAL\n",
    "from optimizer import initialize_v, initialize_s\n",
    "from load import data_loader\n",
    "from metrics import accuracy_score\n",
    "\n",
    "\n",
    "from utils.py_util import dictionary_to_vector, vector_to_dictionary, gradients_to_vector, pd_to_np\n",
    "\n",
    "# %% External module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import math\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacce73-c6b9-4fd7-9b93-033011cfbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['MultilayerPerceptron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5229103-0d75-4322-879a-fd027560a132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11908/2817949453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mMultilayerPerceptron\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"\"\"\n\u001b[0;32m      3\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0mPerceptron\u001b[0m \u001b[0mNeural\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0mAble\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfit\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictors\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0mAble\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpredict_proba\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11908/2817949453.py\u001b[0m in \u001b[0;36mMultilayerPerceptron\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m     def fit(\n\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mEpochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron:\n",
    "    \"\"\"\n",
    "    A L-layer Perceptron Neural Network\n",
    "    - Able to fit with the predictors (X) and the response (Y)\n",
    "    - Able to predict_proba and predict with threshold\n",
    "    \n",
    "    To see the last fit model parameter, uses self.param where self refer to the fit model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_dims : list , hidden_activation_function : str , output_activation_function : str ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Launch the MultilayerPerceptron architecture with the given hyperparameter\n",
    "        \n",
    "        Arguments:\n",
    "         1. layer_dims --- Number of units from input layer to output layer\n",
    "         2. hidden_activation_function --- Activation function for all hidden layer(s) in forward model (relu,leakyrelu,tanh,sigmoid)\n",
    "         3. output_activation_function --- Activation function for all hidden layer(s) in forward model (sigmoid,softmax, linear)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.layer_dims = layer_dims\n",
    "        self.L = len(layer_dims) - 1\n",
    "        self.hidden_activation_function = hidden_activation_function\n",
    "        self.output_activation_function = output_activation_function\n",
    "        \n",
    "        \n",
    "    def compiles(self, lr = 1e-4, loss='binary_cross_entropy_loss', initialization = 'random' ,regularization = None ,\n",
    "                 optimizer = None, **kwargs):\n",
    "        \"\"\"Compile options for training Deep-L layer Neural network\n",
    "        \n",
    "        Arguments\n",
    "        ------------------------\n",
    "        lr --- (initial) learning rate for gradient descent \n",
    "        \n",
    "        loss : str --- loss function of the predicted value and the observation (default : binary_cross_entropy_loss_function)\n",
    "                             [binary_cross_entropy_loss, cross_entropy_loss_function, MSE]\n",
    "                             \n",
    "                        Warning : If your output activation function is Linear, then MSE MUST be your loss\n",
    "                 \n",
    "        initialization --- weight initialization technique (default:random)\n",
    "                             ['zero','random','xavier','he']\n",
    "                            \n",
    "        regularization --- regularization technique (default:None)\n",
    "                             [None, 'L2','dropout']\n",
    "        \n",
    "        optimization (Not done) --- optimization technique (default:None)\n",
    "                             [None, 'momentum' , 'nesterov_momentum' , 'adagrad' , 'rmsprop' , 'adam','nadam']\n",
    "        \n",
    "        Keyword Arguments\n",
    "        ------------------------\n",
    "        lambd ---   L2 Regularization parameter\n",
    "                    *When regularization is 'L2'\n",
    "        \n",
    "        keep_prob_sequence --- Keep probability of the nodes for every layer\n",
    "                                *When regularization is 'Dropout' \n",
    "        \"\"\"\n",
    "        \n",
    "        _allowed_loss = {'binary_cross_entropy_loss','cross_entropy_loss'}\n",
    "        _allowed_initialization = {'zero','random',\n",
    "                                   'xavier','he'}\n",
    "        \n",
    "        _allowed_regularization = {'L2', 'L1',\n",
    "                                   'dropout',\n",
    "                                      None}\n",
    "\n",
    "        _allowed_optimizer = {'momentum','nesterov_momentum',    \n",
    "                              'adagrad','rmsprop',\n",
    "                              'adam','nadam',\n",
    "                                 None}\n",
    "        \n",
    "        assert loss in _allowed_loss,  'Invalid Loss: ' + loss\n",
    "        assert initialization in _allowed_initialization,  'Invalid Initialization: ' + initialization\n",
    "        assert regularization in _allowed_regularization,  'Invalid Regularization: ' + regularization\n",
    "        assert optimizer in _allowed_optimizer,  'Invalid Optimizer: ' + optimizer        \n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        map_loss = {'binary_cross_entropy_loss':binary_cross_entropy_loss, 'cross_entropy_loss' : cross_entropy_loss, 'MSE' : MSE}\n",
    "        map_dZL_loss = {'binary_cross_entropy_loss':BCE_dZL, 'cross_entropy_loss' : CE_dZL, 'MSE' : MSE_dZL}\n",
    "        self.loss_function = map_loss[loss]\n",
    "        self.dZL_loss_function = map_dZL_loss[loss]\n",
    "        self.initialization = initialization\n",
    "        self.regularization = regularization\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        allowed_kwargs = {'lambd',\n",
    "                          'keep_prob_sequence',\n",
    "                          'beta1',\n",
    "                          'use_nesterov',\n",
    "                          'beta2',\n",
    "                          'eps'}\n",
    "        for kwarg in kwargs.keys():                                                 # .. validate all acceptable keyword arguments\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        \n",
    "        if self.regularization:                                                     # .. validate keyword argument for regularization\n",
    "            reg_assertion_dct = {'L2' : {'lambd'},\n",
    "                                 'dropout': {'keep_prob'}}\n",
    "            required = reg_assertion_dct.get(self.regularization)   # {'lambd'}\n",
    "            assert required <= set(kwargs), f'Missing argument {required}'\n",
    "\n",
    "        \n",
    "        if self.optimizer:\n",
    "            optmz_assertion_dct = {'momentum' : {'beta1'},\n",
    "                                   'rmsprop': {'beta2','eps'},\n",
    "                                  'adam' : {'beta1','beta2','eps'}}            \n",
    "            required = optmz_assertion_dct.get(self.optimizer)\n",
    "            assert required <= set(kwargs), f'Missing one of these arguments {required}'        \n",
    "            \n",
    "        self.kwargs_model = kwargs\n",
    "        \n",
    "    def fit(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.Series,\n",
    "        Epochs: int,\n",
    "        batch_size:int=32 ,\n",
    "        seed:int = 42,\n",
    "        report_cost: bool = True,\n",
    "        grad_check:bool = False,\n",
    "        refit: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit the launched Deep L layer with the given data X , Y\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe of predictors\n",
    "         Y --- Pandas Series of response (0 : negative, 1:positive)\n",
    "         Epoch --- number of epochs \n",
    "         batch_size --- Size of batch (Stochastic:1,Mini batch:around 1 and m, Batch: m) \n",
    "         seed --- Random seed for shuffling the row in DataFrame (for non-batch gradient descent)\n",
    "         report_cost --- report the cost epochs every 1000 epoch\n",
    "         grad_check --- Numerically test on the precision of backprop gradient\n",
    "         refit --- reinitialize the parameter and fitting again\n",
    "         \n",
    "        Keyword Argument:\n",
    "         evry_report_epoch = 1000\n",
    "        \"\"\"\n",
    "        allowed_kwargs = {'evry_report_epoch'}\n",
    "        for kwarg in kwargs.keys():                                                 # .. validate all acceptable keyword arguments\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg         \n",
    "            \n",
    "        self.X , self.Y = X, Y ;df = pd.concat([X,Y],axis=1)\n",
    "        mini_batches : list = data_loader(df,batch_size,Y.columns)\n",
    "        if refit:                                                                 # ..Restart fitting ? Initiate the param \n",
    "            self.param = initiate_param(self.layer_dims, self.initialization)\n",
    "            t = 1  \n",
    "            self.kwargs_model.update({'t':t})                                     # kwargs for optimizer \n",
    "        \n",
    "        else:\n",
    "            self.kwargs_model.update({'t':t})  \n",
    "        cost_list = []                                                            # .. for report_cost = True\n",
    "        \n",
    "        if report_cost:\n",
    "            if {'evry_report_epoch'} <= set(kwargs):\n",
    "                evry_report_epoch = kwargs['evry_report_epoch']\n",
    "            else:\n",
    "                evry_report_epoch = 1000\n",
    "                \n",
    "        for epoch in range(1,Epochs+1):                                           # ..Each epoch\n",
    "            cost = 0 \n",
    "            \n",
    "            for batch in range(len(mini_batches)):          \n",
    "                                                                                  \n",
    "                mini_batch = mini_batches[batch]                                  # ..Preprocessing Each batch\n",
    "                mini_batch_X , mini_batch_y = mini_batch[0], mini_batch[1]\n",
    "                \n",
    "                mini_batch_X = mini_batch_X.to_numpy().T\n",
    "                mini_batch_y = mini_batch_y.to_numpy().T\n",
    "                m = mini_batch_y.shape[1]; # Size of each batch\n",
    "                nrow = np.shape(mini_batch_X)[0] # Number of feature\n",
    "                \n",
    "                AL, cache = L_model_forward(mini_batch_X, self.param,                                 # ..Forward Prop\n",
    "                                           hidden_activation_function=self.hidden_activation_function, \n",
    "                                           output_activation_function=self.output_activation_function,\n",
    "                                            **self.kwargs_model )                                      # ..kwargs for dropout\n",
    "\n",
    "                \n",
    "                kwargs_regularization = {'lambd':self.kwargs_model['lambd'], 'param':self.param} if self.regularization in {'L2','L1'} else {}\n",
    "                cost += compute_cost(AL, mini_batch_y,                                           # ..Compute cost function for reporting to user only \n",
    "                                     self.loss_function,\n",
    "                                     self.regularization,\n",
    "                                     **kwargs_regularization)\n",
    "\n",
    "                self.grads = L_model_backward(AL, mini_batch_y, cache,                          # ..Backward Prop                   \n",
    "                                         self.hidden_activation_function,\n",
    "                                        self.output_activation_function,\n",
    "                                        self.dZL_loss_function)\n",
    "\n",
    "                if (grad_check and epoch % evry_report_epoch == 0):                                          # ..Gradient checking for every 1000 epochs\n",
    "                    gradient_check_n(self.param , self.grads, mini_batch_X, mini_batch_y ,\n",
    "                                    self.hidden_activation_function,\n",
    "                                    self.output_activation_function,\n",
    "                                    self.loss_function,\n",
    "                                    self.regularization,\n",
    "                                    **kwargs_regularization)\n",
    "                    \n",
    "\n",
    "                self.kwargs_model.update({'m': m})                                             # ..Update paramater by gradient\n",
    "                \n",
    "                if self.optimizer and self.kwargs_model['t'] == 1:\n",
    "                    self.kwargs_model.update({'v': initialize_v(self.grads)})                 # .. (HELP) required each optimizer to not all have v or s \n",
    "                    self.kwargs_model.update({'s': initialize_s(self.grads)})\n",
    "                \n",
    "                self.param, optmz_cache = update_params(param = self.param, \n",
    "                                                      grads = self.grads, \n",
    "                                                      lr=self.lr,\n",
    "                                                      regularization=self.regularization,\n",
    "                                                      optimizer=self.optimizer,\n",
    "                                                      **self.kwargs_model)                           #..kwargs for L2 regularization\n",
    "                \n",
    "                if self.optimizer: \n",
    "                    v, s = optmz_cache[0], optmz_cache[1]\n",
    "                    t = self.kwargs_model['t'] \n",
    "                    self.kwargs_model.update({ 'v': v, 's': s ,'t':t+1}) \n",
    "            \n",
    "            cost_list.append(np.squeeze(cost)/len(mini_batches))\n",
    "            \n",
    "            if (report_cost and epoch % evry_report_epoch == 0):\n",
    "                print(f\"Epoch {epoch}/{Epochs} : ===Cost=== : {np.squeeze(cost)/len(mini_batches)}\")\n",
    "                \n",
    "        if report_cost:                                                      #..Done fitting and show plot of cost f.\n",
    "            plt.plot(cost_list ,)       \n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Cost function\")\n",
    "            plt.show()\n",
    "    \n",
    "    @pd_to_np\n",
    "    def predict(self,X , predict_proba = True, threshold: float = 0.5):\n",
    "        AL, _ = L_model_forward(X.T, self.param, \n",
    "                                hidden_activation_function=self.hidden_activation_function,\n",
    "                                output_activation_function=self.output_activation_function   # linear if regression\n",
    "                               )\n",
    "        if predict_proba :\n",
    "            AL_ = AL\n",
    "        \n",
    "        else:\n",
    "            if AL.shape[0] == 1 :\n",
    "                AL_ = np.where(AL>=threshold,1,0)\n",
    "                AL_ = np.squeeze(AL_)\n",
    "\n",
    "                assert AL_.ndim == 1\n",
    "\n",
    "                return AL_\n",
    "\n",
    "            elif AL.shape[0] > 1 : \n",
    "                # Multi-class or Multi-label classification\n",
    "                if self.output_activation_function == 'softmax':\n",
    "                    AL_ = np.argmax(AL,axis=0)\n",
    "                    AL_ = self.Y.columns[AL_]\n",
    "                    AL_ = AL_.values\n",
    "\n",
    "                    assert AL_.ndim == 1\n",
    "\n",
    "                elif self.output_activation_function == 'sigmoid':\n",
    "                    print('Under Maintainance for Multi-label')\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                AL_ = AL\n",
    "        \n",
    "        return AL_\n",
    "   \n",
    "    @classmethod\n",
    "    def initiate_by_hyperparam_dict(cls,hyperparam):\n",
    "        \"\"\"\n",
    "        Launch the Deep_L_layer with the given hyperparameter in form of dictionary\n",
    "        \n",
    "        Arguments:\n",
    "        hyperparam: A dictionary with key:\n",
    "        \n",
    "         layer_dims --- Number of units of that L layer\n",
    "         hidden_activation_function --- Activation function for all hidden layer(s) in forward model (relu,leakyrelu,tanh,sigmoid)\n",
    "         last_hidden_activation_function --- Activation function for all hidden layer(s) in forward model (sigmoid,linear)\n",
    "         \n",
    "          {\"layer_dims\" : [ncol,8,6,4,2,1],\n",
    "          \"hidden_activation_function\" : 'tanh' ,\n",
    "          \"output_activation_function\" : 'sigmoid' }\n",
    "          \n",
    "        Supported activation\n",
    "        \"\"\"\n",
    "\n",
    "        # Required hyperparameter attributes\n",
    "        layer_dims = hyperparam['layer_dims']\n",
    "        hidden_activation_function = hyperparam[\"hidden_activation_function\"]\n",
    "        output_activation_function = hyperparam[\"output_activation_function\"]\n",
    "        \n",
    "        return cls(layer_dims,hidden_activation_function,output_activation_function)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MultilayerPerceptron({self.layer_dims}, {self.hidden_activation_function}, {self.output_activation_function})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"A {self.L - 1} Layer Perceptron (Forward activation :{self.hidden_activation_function},Backward activation :{self.last_activation_function})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee88618-53d7-4448-9dfe-af27ed1f2893",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
