{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dc11c8-0c34-4986-a374-02d37530bf10",
   "metadata": {},
   "source": [
    "# Update parameter over iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cd25c-ac5a-4d39-86f5-6146882e4c50",
   "metadata": {},
   "source": [
    "- Gradient descent is used to find the optimal parameter which atleast could make local minimum of cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9210f37f-e80c-465a-ae75-591efb023c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb  # Import modules from Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed3c5f86-78df-44b9-a148-42ecc30ff153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_component import * #linear_forward ,.... linear_backward_model\n",
    "from optimizer import *\n",
    "\n",
    "# %% External modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a74b5e9-3282-450b-bd4b-75d64a6618e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need param , grad, lr\n",
    "# Optimizer method (so generate v & s)\n",
    "# Regularization (add weight decay then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7aceab-c0f2-4e7d-bb9e-bc2811659a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ =   ['update_params', \n",
    "             'initialize_v','initialize_s',\n",
    "             'bias_correction',\n",
    "             'get_momentum_v','get_rmsprop_s','get_adagrad_s','get_adam_v_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540b969a-44d9-4512-bbda-d67faefe7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST OPTIMIZER\n",
    "\n",
    "def update_params(param,grads,lr, regularization=None, optimizer = None, **kwargs):\n",
    "    \"\"\"param in param_ out, updated by gradient descent technique\n",
    "    \n",
    "    Arguments:\n",
    "    1. param\n",
    "    2. grads -- d_theta \n",
    "    3. lr\n",
    "    4. optimizer\n",
    "    5. regularization\n",
    "\n",
    "    Keyword Argument\n",
    "    ----------   \n",
    "    1. lambd \n",
    "    2. beta1\n",
    "    3. beta2\n",
    "    4. epsilon?\n",
    "    \n",
    "    Returns :\n",
    "    param_ : updated parameter\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"dict for dev\n",
    "    grads_ : v\n",
    "    lr_ : scaled lr\n",
    "    param_ : updated param\n",
    "    cache : list of v,s\n",
    "    \"\"\"\n",
    "    L = len(param) // 2\n",
    "\n",
    "    if optimizer: (param, optmz_caches) = _update_params_opt(param,grads,lr,optimizer,**kwargs) #.. special case for optimizer\n",
    "    \n",
    "    else:\n",
    "        for l in range(1,L+1):                                                   #.. No optimizer case \n",
    "            param[\"W\" + str(l)] -= lr * grads[\"dW\" + str(l)]\n",
    "            param[\"b\" + str(l)] -= lr * grads[\"db\" + str(l)]\n",
    "            optmz_caches = None                                                  # ..\n",
    "        \n",
    "    # Add weight decay if regularization\n",
    "\n",
    "    if regularization == \"L2\":\n",
    "        assert {'lambd','m'} <= set(kwargs)\n",
    "        lambd = kwargs.get(\"lambd\")\n",
    "        m = kwargs.get(\"m\")\n",
    "        for l in range(1,L+1):\n",
    "            param[\"W\" + str(l)] -= (lambd/m)*param[\"W\" + str(l)]  \n",
    "\n",
    "    return param, optmz_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0371d9b-c5f4-4cb0-9377-b95873d04397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_params_opt(param,grads,lr,optimizer,**kwargs):\n",
    "    L = len(param) // 2\n",
    "    t = kwargs.get('t')\n",
    "    v = kwargs.get('v')\n",
    "    s = kwargs.get('s')\n",
    "    eps = kwargs.get('eps')\n",
    "\n",
    "    if optimizer == 'momentum':\n",
    "        beta1 = kwargs.get('beta1')\n",
    "        grads_ = get_momentum_v(v,grads,beta1,t)   #..known as v\n",
    "        lr_ : float = lr\n",
    "        \n",
    "        s = None\n",
    "\n",
    "    elif optimizer == 'rmsprop':\n",
    "        beta2 = kwargs.get('beta2')\n",
    "        grads_ = grads\n",
    "        s = get_rmsprop_s(s,grads,beta2,t)\n",
    "        lr_ : dict = get_scaled_lr_rms(s,lr,eps)\n",
    "        \n",
    "    elif optimizer == 'adam':\n",
    "        beta1 = kwargs.get('beta1')\n",
    "        beta2 = kwargs.get('beta2')\n",
    "        grads_ = get_momentum_v(v,grads,beta1,t)\n",
    "        s = get_rmsprop_s(s,grads,beta2,t)\n",
    "        lr_ : dict = get_scaled_lr_rms(s,lr,eps)\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        if not isinstance(lr_,dict) : lr_W, lr_b = np.full_like(grads[\"dW\" + str(l)],lr_), np.full_like(grads[\"db\" + str(l)],lr_)\n",
    "        else: lr_W,lr_b = lr_[ \"dW\" + str(l) ], lr_[ \"db\" + str(l) ]\n",
    "\n",
    "        param[\"W\" + str(l)] -= np.multiply(lr_W, grads_[\"dW\" + str(l)])   #.. element-wise mult\n",
    "        param[\"b\" + str(l)] -= np.multiply(lr_b, grads_[\"db\" + str(l)])\n",
    "    \n",
    "    optmz_caches = [grads_,s]\n",
    "    \n",
    "    return param, optmz_caches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
