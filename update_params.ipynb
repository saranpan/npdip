{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dc11c8-0c34-4986-a374-02d37530bf10",
   "metadata": {},
   "source": [
    "# Update parameter over iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cd25c-ac5a-4d39-86f5-6146882e4c50",
   "metadata": {},
   "source": [
    "- Gradient descent is used to find the optimal parameter which atleast could make local minimum of cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9210f37f-e80c-465a-ae75-591efb023c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb  # Import modules from Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed3c5f86-78df-44b9-a148-42ecc30ff153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_component import * #linear_forward ,.... linear_backward_model\n",
    "\n",
    "# %% External modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a74b5e9-3282-450b-bd4b-75d64a6618e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need param , grad, lr\n",
    "# Optimizer method (so generate v & s)\n",
    "# Regularization (add weight decay then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f7aceab-c0f2-4e7d-bb9e-bc2811659a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ =   ['update_params', \n",
    "             'initialize_v','initialize_s',\n",
    "             'bias_correction',\n",
    "             'get_momentum_v','get_rmsprop_s','get_adagrad_s','get_adam_v_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "540b969a-44d9-4512-bbda-d67faefe7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST OPTIMIZER\n",
    "\n",
    "def update_params(param,grads,lr,optimizer = None ,regularization=None, **kwargs):\n",
    "    \"\"\"param in param_ out, updated by gradient descent technique\n",
    "    \n",
    "    Arguments:\n",
    "    1. param\n",
    "    2. grads -- d_theta \n",
    "    3. lr\n",
    "    4. optimizer\n",
    "    5. regularization\n",
    "\n",
    "    Keyword Argument\n",
    "    ----------   \n",
    "    1. lambd \n",
    "    2. beta1\n",
    "    3. beta2\n",
    "    4. epsilon?\n",
    "    \n",
    "    Returns :\n",
    "    param_ : updated parameter\n",
    "    \"\"\"\n",
    "    _momentum_base_method = {'momentum','nesterov_momentum','adam','nadam'} #create v ( if not have yet ) #require b1 \n",
    "    _adaptive_learning_method = {'adagrad','rmsprop','adam','nadam'}        #create s #require b2, error \n",
    "    \n",
    "    \"\"\"dict for dev\n",
    "    grads_ : v\n",
    "    lr_ : scaled lr\n",
    "    param_ : updated param\n",
    "    \"\"\"\n",
    "    L = len(param) // 2\n",
    "    grads_ = grads\n",
    "    lr_ = lr\n",
    "    #grads_ = v\n",
    "    #lr_ = lr / (np.sqrt(s) + epsilon)\n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] -= lr_ * grads_[\"dW\" + str(l)]\n",
    "        param[\"b\" + str(l)] -= lr_ * grads_[\"db\" + str(l)]\n",
    "        \n",
    "    # Add weight decay if regularization\n",
    "\n",
    "    if regularization == \"L2\":\n",
    "        assert {'lambd','m'} <= set(kwargs)\n",
    "        lambd = kwargs.get(\"lambd\")\n",
    "        m = kwargs.get(\"m\")\n",
    "        for l in range(1,L+1):\n",
    "            param[\"W\" + str(l)] -= (lambd/m)*param[\"W\" + str(l)]   #missing m\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c8e4f2b-7fe8-4b94-9c8a-ee285866833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(param,grads,lr,optimizer = None ,regularization=None, **kwargs):\n",
    "    \"\"\"param in param_ out, updated by gradient descent technique\n",
    "    \n",
    "    Arguments:\n",
    "    1. param\n",
    "    2. grads -- d_theta \n",
    "    3. lr\n",
    "    4. optimizer\n",
    "    5. regularization\n",
    "\n",
    "    Keyword Argument\n",
    "    ----------   \n",
    "    1. lambd \n",
    "    2. beta1\n",
    "    3. beta2\n",
    "    4. epsilon?\n",
    "    \n",
    "    Returns :\n",
    "    param_ : updated parameter\n",
    "    \"\"\"\n",
    "    _momentum_base_method = {'momentum','nesterov_momentum','adam','nadam'} #create v ( if not have yet ) #require b1 \n",
    "    _adaptive_learning_method = {'adagrad','rmsprop','adam','nadam'}        #create s #require b2, error \n",
    "    \n",
    "    \"\"\"dict for dev\n",
    "    grads_ : v\n",
    "    lr_ : scaled lr\n",
    "    param_ : updated param\n",
    "    \"\"\"\n",
    "    L = len(param) // 2\n",
    "    grads_ = grads\n",
    "    lr_ = lr\n",
    "    #grads_ = v\n",
    "    #lr_ = lr / (np.sqrt(s) + epsilon)\n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] -= lr_ * grads_[\"dW\" + str(l)]\n",
    "        param[\"b\" + str(l)] -= lr_ * grads_[\"db\" + str(l)]\n",
    "        \n",
    "    # Add weight decay if regularization\n",
    "\n",
    "    if regularization == \"L2\":\n",
    "        assert {'lambd','m'} <= set(kwargs)\n",
    "        lambd = kwargs.get(\"lambd\")\n",
    "        m = kwargs.get(\"m\")\n",
    "        for l in range(1,L+1):\n",
    "            param[\"W\" + str(l)] -= (lambd/m)*param[\"W\" + str(l)]   #missing m\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbbd61a-f25e-4314-a46b-a0762b4a4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_v(grads):\n",
    "    \"\"\"first iteration of gd with momentum? generate it \n",
    "    \n",
    "    Arguments:\n",
    "    1. grads : grads dict\n",
    "    \n",
    "    Returns :\n",
    "    1. v : initiatized exponentially weighted average of gradient dict\n",
    "    \"\"\"    \n",
    "    \n",
    "    L = len(grads) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        v[\"dW\" + str(l)] = np.zeros_like(grads[\"dW\" + str(l)])\n",
    "        v[\"db\" + str(l)] = np.zeros_like(grads[\"db\" + str(l)])\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d0431b6-9bc0-4626-8500-f03c2e04510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_s(grads):\n",
    "    \"\"\"first iteration of gd with adaptive learning rate? generate it \n",
    "    \n",
    "    Arguments:\n",
    "    1. grads : grads dict\n",
    "    \n",
    "    Returns :\n",
    "    1. v : initiatized exponentially weighted average of squared gradient dict\n",
    "    \"\"\"    \n",
    "    \n",
    "    return initialize_v(grads) # same as initialize_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "357ffd57-0b19-4629-85b0-7ca067f76655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_correction(exp_avg,iteration,beta):\n",
    "    \"\"\"bias correction of v or s due to the exp. weighted average side effect\n",
    "    \n",
    "    Arguments:    \n",
    "    exp_avg --- exponentially weighted average of anything [v/s]\n",
    "    iteration --- current iteration\n",
    "    \n",
    "    Returns:\n",
    "    exp_avg_ --- bias corrected exponentially weighted average of anything \n",
    "    \"\"\"\n",
    "    L = len(exp_avg) // 2 # number of layers in the neural networks\n",
    "    exp_avg_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        exp_avg_[\"dW\" + str(l)] = (exp_avg[\"dW\" + str(l)] / (1-beta**iteration))\n",
    "        exp_avg_[\"db\" + str(l)] = (exp_avg[\"db\" + str(l)] / (1-beta**iteration))\n",
    "    \n",
    "    return exp_avg_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8bb97-8613-4149-803b-544a3b51716a",
   "metadata": {},
   "source": [
    "## Optimizer with Momentum\n",
    "\n",
    "<fieldset>\n",
    "    \n",
    "- Momentum ✔️\n",
    "- Nesterov accerelated Gradient (aka. Nesterov Momentum) ❌\n",
    "    \n",
    "</fieldset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b786d0-535f-4ed8-a795-b21e6a49910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_momentum_v(v,grads,beta1,iteration):\n",
    "    \"\"\"grads in exp.grads out \n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    1. v\n",
    "    2. grads\n",
    "    3. beta1\n",
    "    4. nesterov\n",
    "    \n",
    "    Returns :\n",
    "    1. s_ : new exponentially weighted average of gradient \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(grads) // 2 # number of layers in the neural networks\n",
    "    v_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        v_[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1-beta1) * grads[\"dW\" + str(l)]\n",
    "        v_[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1-beta1) * grads[\"db\" + str(l)]\n",
    "    \n",
    "    if iteration <= 10:\n",
    "        ### Bias correction ###\n",
    "        v_ = bias_correction(v_,iteration,beta1)\n",
    "        \n",
    "    return v_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d594a46-be9d-4c7c-a0ae-73e91f029cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nesterov_momentum_v(v,param):\n",
    "    \"\"\"grads lookahead in exp of grads lookahead oiut\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dc9a0-3d56-4685-9265-55623e9ef358",
   "metadata": {},
   "source": [
    "## Optimizer with Adaptive learning method\n",
    "\n",
    "<fieldset>\n",
    "    \n",
    "- Adagrad ✔️\n",
    "- RMSProp ✔️ <br>\n",
    "- AdaDelta ❌\n",
    "    \n",
    "</fieldset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daa97c36-930e-4baf-abc5-802ae89f7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adagrad_s(s,grads):\n",
    "    \"\"\"grads in exp.grads^2 out \n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    1. s\n",
    "    2. grads\n",
    "    \n",
    "    Returns :\n",
    "    1. s_ : new exponentially weighted average of (gradient)^2\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(grads) // 2 # number of layers in the neural networks\n",
    "    s_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        s_[\"dW\" + str(l)] += grads[\"dW\" + str(l)]\n",
    "        s_[\"db\" + str(l)] += grads[\"db\" + str(l)]\n",
    "    \n",
    "    return s_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ec507c8-499e-4ce7-9960-94702085aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsprop_s(s,grads,beta2,iteration):\n",
    "    \"\"\"grads in exp. grads^2 out \n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    1. s\n",
    "    2. grads\n",
    "    3. beta2\n",
    "    4. iteration --- current iteration for bias correction\n",
    "    \n",
    "    Returns :\n",
    "    1. s_ : new exponentially weighted average of (gradient)^2\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(grads) // 2 # number of layers in the neural networks\n",
    "    s_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        s_[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1-beta2) * (grads[\"dW\" + str(l)])**2\n",
    "        s_[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1-beta2) * (grads[\"db\" + str(l)])**2\n",
    "    \n",
    "    if iteration <= 10:\n",
    "        ### Bias correction ###\n",
    "        s_ = bias_correction(s_,iteration,beta2)\n",
    "    \n",
    "    return s_  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbece5-a4b4-4aec-9080-826e0c516d46",
   "metadata": {},
   "source": [
    "## Optimizer with Momentum & Adaptive learning method\n",
    "\n",
    "<fieldset>\n",
    "    \n",
    "- Adam ✔️\n",
    "- Nadam ❌<br>\n",
    "    \n",
    "</fieldset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d51cce3-5cac-4101-8e14-c3457c3de0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adam_v_s(v,beta1,s,beta2,iteration):\n",
    "    \"\"\" Obtain v and s of Adam\n",
    "    \n",
    "    Arguments:\n",
    "    1. v\n",
    "    2. beta1\n",
    "    3. s\n",
    "    4. beta2\n",
    "    5. iteration --- current iteration for bias correction\n",
    "    \n",
    "    Returns :\n",
    "    1. v_ : new exponentially weighted average of gradient\n",
    "    2. s_ : new exponentially weighted average of (gradient)^2\n",
    "    \"\"\"\n",
    "    v_ = get_momentum_v(v,grads,beta1,iteration)\n",
    "    s_ = get_rmsprop_s(s,grads,beta2,iteration)\n",
    "    return v_ , s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9482fe23-0303-4e67-a19e-3715236d2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nadam_v_s(v,beta1,s,beta2,iteration):\n",
    "    v_ = get_nesterov_momentum_v(v,grads,beta1,iteration)\n",
    "    s_ = get_rmsprop_s(s,grads,beta2,iteration)\n",
    "    return v_ , s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20423f50-d895-4bf4-a35f-1c31ea8f0eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'L_model_backward',\n",
       " 'L_model_forward',\n",
       " 'Out',\n",
       " '_',\n",
       " '_24',\n",
       " '_3',\n",
       " '_4',\n",
       " '__',\n",
       " '___',\n",
       " '__all__',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i23',\n",
       " '_i24',\n",
       " '_i25',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'bias_correction',\n",
       " 'exit',\n",
       " 'get_adagrad_s',\n",
       " 'get_adam_v_s',\n",
       " 'get_ipython',\n",
       " 'get_momentum_v',\n",
       " 'get_nadam_v_s',\n",
       " 'get_nesterov_momentum_v',\n",
       " 'get_rmsprop_s',\n",
       " 'import_ipynb',\n",
       " 'initialize_s',\n",
       " 'initialize_v',\n",
       " 'linear_activation_backward',\n",
       " 'linear_activation_forward',\n",
       " 'linear_backward',\n",
       " 'linear_forward',\n",
       " 'np',\n",
       " 'quit',\n",
       " 'statistics',\n",
       " 'update_params']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dir()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643a8a2-9e6d-4d4b-a211-6ebfd4df5d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
