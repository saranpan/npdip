{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1e0253-5ff3-49d5-8723-402fddb9b26a",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "- Make parameter goes toward local minima quicker, and be able to escape saddle point by adjusting Gradient descent with the following views\n",
    "\n",
    "1. gradient into exponentially weighted average of gradient (to go toward minima quickly)\n",
    "2. learning rate into scaled learning rate (to be able to escape the saddle & converge to minima when the gradient is)\n",
    "\n",
    "- There are many optimizers from momentum to ndam\n",
    "\n",
    "1. Momentum (1)\n",
    "2. Nesterov momentum (1)\n",
    "3. AdaGrad (2)\n",
    "4. RMSProp (2)\n",
    "\n",
    "- Combination of these above optimizer\n",
    "1. adam (1) (4)\n",
    "2. nadam (2) (4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1bf49-3206-4a67-a481-c5a85020e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% External modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c6c84-2b25-435c-b4b6-12acef06dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['initialize_v','initialize_s','bias_correction',\n",
    "          'get_momentum_v','get_adagrad_s','get_rmsprop_s','get_scaled_lr_rms','get_adam_v_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722983be-aced-4647-ad4f-0ecee16df0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_v(grad):\n",
    "    \"\"\"first iteration of gd with momentum? generate it \n",
    "    \n",
    "    Arguments:\n",
    "    1. grad : grad dict\n",
    "    \n",
    "    Returns :\n",
    "    1. v : initiatized exponentially weighted average of gradient dict\n",
    "    \"\"\"    \n",
    "    \n",
    "    L = len(grad) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        v[\"dW\" + str(l)] = np.zeros_like(grad[\"dW\" + str(l)])\n",
    "        v[\"db\" + str(l)] = np.zeros_like(grad[\"db\" + str(l)])\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfd38c-cce3-4917-8afb-0463c07ad054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_s(grad):\n",
    "    \"\"\"first iteration of gd with adaptive learning rate? generate it \n",
    "    \n",
    "    Arguments:\n",
    "    1. grad : grad dict\n",
    "    \n",
    "    Returns :\n",
    "    1. v : initiatized exponentially weighted average of squared gradient dict\n",
    "    \"\"\"    \n",
    "    \n",
    "    return initialize_v(grad) # same as initialize_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22410b13-9423-4246-813a-be21acb14a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_correction(exp_avg,iteration,beta):\n",
    "    \"\"\"bias correction of v or s due to the exp. weighted average side effect\n",
    "    \n",
    "    Arguments:    \n",
    "    exp_avg --- exponentially weighted average of anything [v/s]\n",
    "    iteration --- current iteration\n",
    "    beta --- hyperparameter of that exp_avg\n",
    "    \n",
    "    Returns:\n",
    "    exp_avg_ --- bias corrected exponentially weighted average of anything \n",
    "    \"\"\"\n",
    "    L = len(exp_avg) // 2 # number of layers in the neural networks\n",
    "    exp_avg_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        exp_avg_[\"dW\" + str(l)] = (exp_avg[\"dW\" + str(l)] / (1-beta**iteration))\n",
    "        exp_avg_[\"db\" + str(l)] = (exp_avg[\"db\" + str(l)] / (1-beta**iteration))\n",
    "    \n",
    "    return exp_avg_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af84f9-7a49-449e-9752-bed8e308c68e",
   "metadata": {},
   "source": [
    "# 1. Gradient to Exponentially weighted average of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01a6cc-ce20-4a93-b88f-2a1443e682c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_momentum_v(v,grad,beta1,iteration):\n",
    "    \"\"\"grad in exp.grad out \n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    1. v\n",
    "    2. grad\n",
    "    3. beta1\n",
    "    4. nesterov\n",
    "    \n",
    "    Returns :\n",
    "    1. s_ : new exponentially weighted average of gradient \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(grad) // 2 # number of layers in the neural networks\n",
    "    v_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        v_[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1-beta1) * grad[\"dW\" + str(l)]\n",
    "        v_[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1-beta1) * grad[\"db\" + str(l)]\n",
    "    \n",
    "    if iteration <= 10:\n",
    "        ### Bias correction ###\n",
    "        v_ = bias_correction(v_,iteration,beta1)\n",
    "        \n",
    "    return v_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0fcb1b-83e8-4819-954a-3da0b776f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nesterov_momentum_v(v,param):\n",
    "    \"\"\"grad lookahead in exp of grad lookahead oiut\n",
    "    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e557c-41a4-48d3-a1df-0e9f14435697",
   "metadata": {},
   "source": [
    "# 2. Learning rate to Scaled learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0169e8d-c92c-4f61-a0de-3268676b6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adagrad_s(s,grad):\n",
    "    \"\"\"grad in exp.grad^2 out \n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    1. s\n",
    "    2. grad\n",
    "    \n",
    "    Returns :\n",
    "    1. s_ : new exponentially weighted average of (gradient)^2\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(grad) // 2 # number of layers in the neural networks\n",
    "    s_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        s_[\"dW\" + str(l)] += grad[\"dW\" + str(l)]\n",
    "        s_[\"db\" + str(l)] += grad[\"db\" + str(l)]\n",
    "    \n",
    "    return s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd96d103-1909-411f-af33-fad1bfbc5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsprop_s(s,grad,beta2,iteration):\n",
    "    \"\"\"grad in exp. grad^2 out \n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    1. s\n",
    "    2. grad\n",
    "    3. beta2\n",
    "    4. iteration --- current iteration for bias correction\n",
    "    \n",
    "    Returns :\n",
    "    1. s_ : new exponentially weighted average of (gradient)^2\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(grad) // 2 # number of layers in the neural networks\n",
    "    s_ = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        s_[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1-beta2) * (grad[\"dW\" + str(l)])**2\n",
    "        s_[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1-beta2) * (grad[\"db\" + str(l)])**2\n",
    "    \n",
    "    if iteration <= 10:\n",
    "        ### Bias correction ###\n",
    "        s_ = bias_correction(s_,iteration,beta2)\n",
    "    \n",
    "    return s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93306c2-8ca6-4586-af03-75bb26d8e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_lr_rms(s, lr, eps):\n",
    "    \n",
    "    L = len(s) // 2\n",
    "    lr_dct = {}\n",
    "    for l in range(1,L+1):\n",
    "        lr_dct[ \"dW\" + str(l) ] = np.full_like( s[\"dW\" + str(l)],lr )\n",
    "        lr_dct[ \"dW\" + str(l) ] /= (np.sqrt(s[\"dW\" + str(l)]) + eps)\n",
    "        \n",
    "        lr_dct[ \"db\" + str(l) ] = np.full_like( s[\"db\" + str(l)],lr )\n",
    "        lr_dct[ \"db\" + str(l) ] /= (np.sqrt(s[\"db\" + str(l)]) + eps)\n",
    "    \n",
    "    return lr_dct  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a5608-4a0b-40b3-ad97-a29edcd7e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adam_v_s(v,beta1,s,beta2,iteration):\n",
    "    \"\"\" Obtain v and s of Adam\n",
    "    \n",
    "    Arguments:\n",
    "    1. v\n",
    "    2. beta1\n",
    "    3. s\n",
    "    4. beta2\n",
    "    5. iteration --- current iteration for bias correction\n",
    "    \n",
    "    Returns :\n",
    "    1. v_ : new exponentially weighted average of gradient\n",
    "    2. s_ : new exponentially weighted average of (gradient)^2\n",
    "    \"\"\"\n",
    "    v_ = get_momentum_v(v,grad,beta1,iteration)\n",
    "    s_ = get_rmsprop_s(s,grad,beta2,iteration)\n",
    "    return v_ , s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a569cd-65f7-4472-b152-cc7ece77a644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nadam_v_s(v,beta1,s,beta2,iteration):\n",
    "    v_ = get_nesterov_momentum_v(v,grad,beta1,iteration)\n",
    "    s_ = get_rmsprop_s(s,grad,beta2,iteration)\n",
    "    return v_ , s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ddf49d-c6b8-4a7d-af2b-f0a64ec1cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4c9087-f25f-4e23-9520-ba5151308e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    #self.iter = 1?\n",
    "    def __init__(self,beta1,temp_param): # retreive **kwargs_dict - lambd\n",
    "        self.v = initialize_v(temp_param)\n",
    "        self.iteration = 1\n",
    "        \n",
    "    def get_grad(self,grad,v) -> dict :   \n",
    "\n",
    "        grad_ = get_momentum_v(self.v,\n",
    "                               grad,\n",
    "                               beta1,\n",
    "                               self.iteration)\n",
    "        \n",
    "        self.iteration += 1\n",
    "        return grad_\n",
    "    \n",
    "    def get_lr(self,grad,lr): \n",
    "        return lr\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34f6f471-7b25-4673-8c11-05792fec2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rmsprop(Optimizer):\n",
    "    \n",
    "    def __init__(self,beta2,eps,temp_param):\n",
    "        self.s = initialize_s(temp_param)\n",
    "        self.iteration = 1\n",
    "        \n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "    \n",
    "    def get_grad(self,grad): \n",
    "        return grad\n",
    "\n",
    "    def get_lr(self,grad,lr) -> dict :\n",
    "        s = get_rmsprop_s(self.s, \n",
    "                          grad, \n",
    "                          self.beta2, \n",
    "                          self.eps, \n",
    "                          self.iteration)\n",
    "        lr_ : dict = get_scaled_lr_rms(s,lr,eps)\n",
    "        self.iteration += 1\n",
    "        \n",
    "        return lr_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b5114-b9d1-427a-8f6b-38dfd67d0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    \n",
    "    def __init__(self,beta1,beta2,eps,temp_param):\n",
    "        self.v = initialize_v(temp_param)\n",
    "        self.s = initialize_s(temp_param)\n",
    "        self.iteration = 1\n",
    "        \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "    \n",
    "    def get_grad(self,grad) -> dict :   \n",
    "        grad_ = get_momentum_v(self.v,\n",
    "                           grad,\n",
    "                           self.beta1,\n",
    "                           self.iteration)\n",
    "        return grad_\n",
    "    \n",
    "    def get_lr(self,grad,lr) -> dict :\n",
    "        s = get_rmsprop_s(self.s, \n",
    "                          grad, \n",
    "                          self.beta2, \n",
    "                          self.eps, \n",
    "                          self.iteration)\n",
    "        lr_ : dict = get_scaled_lr_rms(s,lr,eps)\n",
    "        self.iteration += 1\n",
    "        \n",
    "        return lr_ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
