{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdd7032-5cfd-4b49-9e88-f1bb2117cecb",
   "metadata": {},
   "source": [
    "# Weight Initialization\n",
    "\n",
    "- `mlp` updates their parameter from gradient (gradient descent), without initiated parameter, it would say \"Gradient of what?\"\n",
    "\n",
    "- The problem of `initialization_zero` for mlp makes all units in every layer got updated by the same gradient, when all units got the same updated then it means it's simply a model with every layer by 1 unit\n",
    "\n",
    "- `initialization_random` figures out that zero problem, so each unit is uniquely updated but stuck at vanishing/exploding gradient problem\n",
    "- `initialization_xavier` figures out that random problem, appropriate for TanH activation function\n",
    "- `initialization_he` figures out that random problem, appropriate for ReLU and its variant activation function\n",
    "\n",
    "Both xavier and he does not specify the distribution of parameter, so we planned to get both normal and **uniform (not done)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f376f4-e644-4383-bd92-99d8510a174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% External module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e76b3-a841-40fb-8f68-5d38953e2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ =   ['initiate_param', \n",
    "             'initialization_zero','initialization_random',\n",
    "             'initialization_xavier','initialization_he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00c59bd-63f3-4138-acda-d5152cda5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAN TO REPLACE AS DECORATOR\n",
    "\n",
    "def initiate_param(layer_dims:list ,initialization :str = 'random',seed:int = 42) -> list:\n",
    "    \"\"\"Initiate the paramaters W, B for each layer\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "        layer_dims : list\n",
    "            A sequence of number of units for every layer \n",
    "        initialization : str, optional\n",
    "            A technique of weight initialization (default:random)\n",
    "        seed : int, optional \n",
    "            A seed for randomize the initialization\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        param : numpy.array\n",
    "            Array of parameter of every layer \n",
    "    \n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    allowed_initialization_method = {'zero': initialization_zero,\n",
    "                                     'random' : initialization_random,\n",
    "                                    'he' : initialization_he,\n",
    "                                    'xavier' : initialization_xavier}\n",
    "    \n",
    "    initialization_method = allowed_initialization_method[initialization]\n",
    "    param = initialization_method(layer_dims)\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3624af7-d5c2-4708-ade8-e062348eb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_zero(layer_dims:list):\n",
    "    \"\"\"Initialize both weight and bias as zeros\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dims : int\n",
    "        A sequence of number of units for every layer \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = np.zeros(shape=(layer_dims[l], layer_dims[l-1])) * 0.01 # Uniform(0,1] * 0.01\n",
    "        param[\"b\" + str(l)] = np.zeros(shape=(layer_dims[l], 1))\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ef80b9-0485-4893-8ea8-f6d1500fd878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_random(layer_dims:list,scale:int=0.01):\n",
    "    \n",
    "    \"\"\"Initialize weight randomly with Normal(mean=0,sigma=1)\n",
    "    Initialize bias as uniform distributed ( min=0,max= <1 )\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dimss : int\n",
    "        A sequence of number of units for every layer \n",
    "    scale : float, optional\n",
    "        A constant to scale the weight initialization\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    \"\"\"\n",
    "    scale : variance of the random variable\n",
    "    y = scale * x\n",
    "    var(y) = var(scale*x)\n",
    "    var(y) = scale^2 * x\n",
    "    \"\"\"\n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * scale # Normal(0,1) * scale \n",
    "        param[\"b\" + str(l)] = np.random.rand(layer_dims[l], 1)\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffcfe63-00bf-47bc-9536-07b7dee1c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_xavier(layer_dims:list):\n",
    "    \"\"\"\n",
    "    Initialize weight randomly with Normal(mean=0,sigma=(1/fan_avg))\n",
    "    Initialize bias as uniform distributed ( min=0,max= <1 )\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dimss : int\n",
    "        A sequence of number of units for every layer \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        fan_in , fan_out = layer_dims[l-1] , layer_dims[l]\n",
    "        fan_avg = 1/2 * (fan_in + fan_out)\n",
    "        \n",
    "        param[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1/fan_avg) \n",
    "        param[\"b\" + str(l)] =  np.random.rand(layer_dims[l], 1)\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bdefa-6cdc-442b-9929-7b6afc98f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_he(layer_dims:list):\n",
    "    \"\"\"\n",
    "    Initialize weight randomly with Normal(mean=0,sigma=(2/fan_in))\n",
    "    Initialize bias as uniform distributed ( min=0,max= <1 )\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dimss : int\n",
    "        A sequence of number of units for every layer \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        fan_in = layer_dims[l-1]\n",
    "        \n",
    "        param[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/fan_in) \n",
    "        param[\"b\" + str(l)] =  np.random.rand(layer_dims[l], 1)\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
